{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKnXpX_D0WHg"
      },
      "source": [
        "## Random Forest classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F9WjPA3jT7_"
      },
      "source": [
        "Install pyspark library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW_tW2KBwzvt",
        "outputId": "6873141c-17f9-4938-ccae-ddc3f9ccca89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=eef230613ef96a9ad6556c765a990b9fbd954f700ef25231cf9227b456e49e98\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQihddVElwRW",
        "outputId": "6f4e5ebc-43b4-4eba-af39-01f4d843cfdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_9f79z2xWWA",
        "outputId": "46a4c1a0-360b-43ba-edb5-bbfac515362a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cq7eV0xmmAp"
      },
      "source": [
        "A Spark session is encapsulated in an instance of org.apache.spark.sql.SparkSession. The session object has information about the Spark Master, the Spark application, and the configuration options.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meO9dRi8NH-B"
      },
      "outputs": [],
      "source": [
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epe1FF_BmWcQ"
      },
      "source": [
        "RDD is a parallelised data structure that gets the workload distributed across the worker nodes. They are the basic units of Spark programming. To work with RDDs, we need to create a SparkContext first.\n",
        "A SparkContext is the entry gate for Spark environment. For every Sparkapp you need to create the SparkContext object. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. Now, we need to create SparkContext:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc4vjhNgmXpC"
      },
      "outputs": [],
      "source": [
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH4NZfWK3vDw"
      },
      "source": [
        "Next, we will import the dataset using read.csv function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLGIycFC3wht",
        "outputId": "324306aa-a8af-4d0e-9b2f-a41c69622494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- State: string (nullable = true)\n",
            " |-- Agency_type: string (nullable = true)\n",
            " |-- Solved: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Report_status: string (nullable = true)\n",
            " |-- Crime_type: string (nullable = true)\n",
            " |-- Crime_status: string (nullable = true)\n",
            " |-- Victim_age: integer (nullable = true)\n",
            " |-- Victim_sex: string (nullable = true)\n",
            " |-- Victim_race: string (nullable = true)\n",
            " |-- Offender_age: integer (nullable = true)\n",
            " |-- Offender_sex: string (nullable = true)\n",
            " |-- Offender_race: string (nullable = true)\n",
            " |-- Weapon: string (nullable = true)\n",
            " |-- Relationship: string (nullable = true)\n",
            " |-- Crime_cause: string (nullable = true)\n",
            " |-- Victim_prior_offense_status: string (nullable = true)\n",
            " |-- add_victim_count: integer (nullable = true)\n",
            " |-- add_offender_count: integer (nullable = true)\n",
            " |-- County: string (nullable = true)\n",
            " |-- Offender_demo: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('/content/cleaned_data.csv', header = True, inferSchema = True)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3hicUIqSstO"
      },
      "source": [
        "## PySpark and Machine Learning\n",
        "\n",
        "For the Machine Learning part, we use Pipeline, which is an optimization method, to chain multiple Transformers and Estimators together to specify our machine learning workflow that Spark uses to improve the performance of computations.\n",
        "Using Random Forest Method in PySpark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3yWuKu39moS"
      },
      "source": [
        "Random forest method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0kUOqVXI3cqD",
        "outputId": "d2c2f207-0914-4963-fa7d-df23e42f6e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+\n",
            "|Offender_demo|prediction|\n",
            "+-------------+----------+\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55F|       2.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|         <18M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "|       19-55M|       0.0|\n",
            "+-------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test Accuracy = 89.36%\n",
            "StringIndexerModel: uid=StringIndexer_91cb12ce02f0, handleInvalid=keep\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Initializing SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Random Forest Classifier\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Convertin string columns to numerical using StringIndexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in df.columns if column != \"Offender_demo\"]\n",
        "\n",
        "# StringIndexer for the target variable\n",
        "target_indexer = StringIndexer(inputCol=\"Offender_demo\", outputCol=\"Offender_demo_index\", handleInvalid=\"keep\")\n",
        "\n",
        "# Assembling feature vector\n",
        "assembler = VectorAssembler(inputCols=[column+\"_index\" for column in df.columns if column != \"Offender_demo\"], outputCol=\"features\")\n",
        "\n",
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Defining Random Forest Classifier with increased maxBins\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Offender_demo_index\", maxBins=2000)\n",
        "\n",
        "# Creating Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [target_indexer,assembler, rf])\n",
        "\n",
        "# Fiting the pipeline to the data\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"Offender_demo\", \"prediction\").show()\n",
        "\n",
        "\n",
        "# Selecting (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Offender_demo_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Accuracy = {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "rfModel = model.stages[2]\n",
        "print(rfModel)  # summary only"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}